{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b93b6fd8-d0fc-467e-a7c1-07bb307cac77",
   "metadata": {},
   "source": [
    "# EfficientNet_B0\n",
    "\n",
    "Vamos a cargar el modelo preentrenado con el conjunto de datops de ImageNet. Congelaremos los parámetros del modelo preentrenado y modificaremos la capa de clasificación final: Durante el entrenamiento únicamente se modificarán loa parámetros de la capa de clasificación.\n",
    "\n",
    "Sustituimos el clasificador lineal por una secuencia de 3 capas de lineales más sus capas de activación ReLU y una capa dropout en el medio.\n",
    "\n",
    "## Importamos librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70a802e-564a-4413-b655-cc90dae87264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict, List\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f294fc-544a-42c7-ae03-ac8f11e79242",
   "metadata": {},
   "source": [
    "## Definimos parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629794a1-c856-4e35-85fe-308eab07af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un diccionario vacio para almacenar los resultados\n",
    "results = {'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "cudnn.benchmark = True\n",
    "\n",
    "data_dir = './data'\n",
    "img_size = (224, 224)\n",
    "bs = 16\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "num_epochs = 25\n",
    "output_classes = 10000\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba87ae21-0fba-4b9d-9b31-a5aec923a47d",
   "metadata": {},
   "source": [
    "## Definimos la transformación\n",
    "\n",
    "Definimos las diferentes transformaciones que se van a aplicar previamente sobre los datos:\n",
    "\n",
    "* Redimensionamos todas las imágenes al mismo tamaño (224x224).\n",
    "* Aleatoriamente se aplica un giro horizontal de la imagen.\n",
    "* Rotamos la imagen 10 grados de forma aleatoria.\n",
    "* Los trasformaremos en tensores y normalizamos $\\frac{x-mean}{std}$ centrado en 0 y [-1, 1].\n",
    "\n",
    "Inicialmente, vamos a aplicar la misma transformación al conjunto de datos de entrenamiento que al conjunto de datos de validación. Pero hemos escrito el código de forma que se puedan aplicar diferentes transformaciones al conjunto de datos de entrenamiento que al conjunto de datos de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bfd771-efb9-4d2c-8f8e-09cd45804fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86af4593-22ce-46da-bfbc-db94f85fbb05",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10821160-78ed-48d6-92f2-18503eaa9a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {\n",
    "    x: datasets.INaturalist(\n",
    "        root = data_dir,\n",
    "        version = '2021_train_mini' if x == 'train' else '2021_valid',\n",
    "        transform = data_transforms[x],\n",
    "        download = False\n",
    "    )\n",
    "    for x in ['train', 'val']\n",
    "}\n",
    "dataloaders = {\n",
    "    x: torch.utils.data.DataLoader(\n",
    "        image_datasets[x],\n",
    "        batch_size = bs,\n",
    "        shuffle = True,\n",
    "        num_workers = 4\n",
    "    )\n",
    "    for x in ['train', 'val']\n",
    "}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "print(f'Conjuntos de entrenamiento: {dataset_sizes} clases: {output_classes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc358cd-be96-44b9-82c9-ae6eed5cec8b",
   "metadata": {},
   "source": [
    "## Función de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8335139-f3d5-4198-bc74-60db26bf742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=20):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Entrenando etapa {epoch + 1} de {num_epochs} ...')\n",
    "        epoch_init = time.time()\n",
    "\n",
    "        # Cada iteración tiene una fase de validación y una fase de entrenamiento\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Establece el modelo en modo entrenamiento\n",
    "            else:\n",
    "                model.eval()   # Establece el modelo en modo validación\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Itera sobre los datos\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # pone a cero los gradientes de los tensores optimizados\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # track historial adelante solo en la fase de entrenamiento\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Solo si está en la fase de entrenamiento retroceder + optimizar\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # estatisticas\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            if phase == 'train':\n",
    "                results[\"train_loss\"].append(epoch_loss)\n",
    "                results[\"train_acc\"].append(epoch_acc.item())\n",
    "                print(f'Entrenamiento\\tPerdida: {epoch_loss:.4f}\\tPrecisión: {epoch_acc:.4f}')\n",
    "            else:\n",
    "                tittle = 'Validación'\n",
    "                results[\"val_loss\"].append(epoch_loss)\n",
    "                results[\"val_acc\"].append(epoch_acc.item())\n",
    "                print(f'Validación\\tPerdida: {epoch_loss:.4f}\\tPrecisión: {epoch_acc:.4f}'\n",
    "                      f'\\t{(time.time() - epoch_init)//60:.0f}min {(time.time() - epoch_init)%60:.0f}seg')\n",
    "            \n",
    "            # copia del modelo con mejor porcentaje de acierto en la validación\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                print('Guardando el modelo ...')\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    if time_elapsed < 3600:\n",
    "        print(f'Entrenamiento completado en: {time_elapsed // 60:.0f}min. {time_elapsed % 60:.0f}seg.')\n",
    "    else:\n",
    "        rest_elapsed = time_elapsed % 3600\n",
    "        print(f'Entrenamiento completado en: {time_elapsed // 3600:.0f}horas {rest_elapsed // 60:.0f}min. {rest_elapsed % 60:.0f}seg.')\n",
    "    print(f'Mejor precisión validación: {best_acc:.4f}')\n",
    "\n",
    "    # guarda los mejores pesos del modelo\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a826bd-9e57-4ec0-8a4a-cb0507b28767",
   "metadata": {},
   "source": [
    "## Modelo preentrenado EfficientNet-B0\n",
    "\n",
    "Cargamos el modelo y congelamos los parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc933a6-1832-4e59-9f7b-64c502b96e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.efficientnet_b0(weights='IMAGENET1K_V1')\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cc70d9-8f94-410e-a6ed-98abb4d9f6de",
   "metadata": {},
   "source": [
    "Sustituimos el clasificador lineal por una secuencia de 3 capas de lineales más sus capas de activación ReLU y una capa dropout en el medio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0c534a-59fa-4cb5-9a5d-4a08acee549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ftrs = model.classifier[1].in_features\n",
    "\n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "    ('0', nn.Linear(num_ftrs, 625)),\n",
    "    ('1', nn.ReLU(inplace=True)),\n",
    "    ('2', nn.Dropout(p=0.3)),\n",
    "    ('3', nn.Linear(625, 256)),\n",
    "    ('4', nn.ReLU(inplace=True)),\n",
    "    ('5', nn.Linear(256, output_classes)),\n",
    "]))\n",
    "\n",
    "model.classifier = classifier\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Se optimizan todos los parámetros\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Se decrementa LR por un factor 0.1 cada 7 iteraciones\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2018e97-c415-4ef5-a2be-02bd09c6af61",
   "metadata": {},
   "source": [
    "## Llamamos a la función de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2c813c-2c33-4c7d-84b4-7e332324cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148c8124-3e28-445a-985a-32afcc7b1aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
