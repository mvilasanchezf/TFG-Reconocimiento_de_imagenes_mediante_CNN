{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "465239b6-2922-4ef7-b462-7e3c57f67236",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "\n",
    "## 1. Importamos librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81eec07a-cdf4-422f-af36-e2ca6262bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb3b6b69-2422-4256-8162-797a539d4651",
   "metadata": {},
   "source": [
    "## 2. Definimos parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aecc8f2-6c01-404f-844b-7a18d078dd15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x2047da2f6d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = './data'\n",
    "img_size = (300, 300)\n",
    "bs = 32\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Creamos un diccionario vacio para almacenar los resultados\n",
    "results = {'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # modo interactivo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "646466c2-05b7-4efa-b0e5-9bd6076569ec",
   "metadata": {},
   "source": [
    "## Definimos el parámetro transform\n",
    "\n",
    "Vamos a cargar el conjunto de datos y sobre estos datos vamos a aplicar una serie de transformaciones previas:\n",
    "\n",
    "* Redimensionamos todas las imágenes al mismo tamaño (224x224).\n",
    "* Aleatoriamente se aplica un giro horizontal de la imagen.\n",
    "* Rotamos la imagen 10 grados de forma aleatoria.\n",
    "* Los trasformaremos en tensores y normalizamos $\\frac{x-mean}{std}$\n",
    "\n",
    "Inicialmente, vamos a aplicar la misma transformación al conjunto de datos de entrenamiento que al conjunto de datos de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64911e06-2c02-4a21-83fe-ff84d4e10496",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab2f9773-9736-4517-8ac4-82355c3aadb1",
   "metadata": {},
   "source": [
    "### Carga los datos de iNaturalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc0a4b75-98de-40d0-8c87-270f067f30d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {\n",
    "    x: datasets.INaturalist(\n",
    "        root = data_dir,\n",
    "        version = '2021_train_mini' if x == 'train' else '2021_valid',\n",
    "        transform = data_transforms[x],\n",
    "        download = False\n",
    "    )\n",
    "    for x in ['train', 'val']\n",
    "}\n",
    "dataloaders = {\n",
    "    x: torch.utils.data.DataLoader(\n",
    "        image_datasets[x],\n",
    "        batch_size = bs,\n",
    "        shuffle = True,\n",
    "        num_workers = 4\n",
    "    )\n",
    "    for x in ['train', 'val']\n",
    "}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "inputs, classes = next(iter(dataloaders['train']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68af2970-6497-433b-abdc-a84346a76a8c",
   "metadata": {},
   "source": [
    "## Definimos la función de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef0f9a52-906c-4201-b860-c63c75ea628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Etapa {epoch + 1}/{num_epochs}')\n",
    "        print('-' * 11)\n",
    "\n",
    "        # Cada iteración tiene una fase de validación y una fase de entrenamiento\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Establece el modelo en modo entrenamiento\n",
    "            else:\n",
    "                model.eval()   # Establece el modelo en modo validación\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Itera sobre los datos\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # pone a cero los gradientes de los tensores optimizados\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # track historial adelante solo en la fase de entrenamiento\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Solo si está en la fase de entrenamiento retroceder + optimizar\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # estatisticas\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            if phase == 'train':\n",
    "                tittle = 'Entrenamiento'\n",
    "                results[\"train_loss\"].append(epoch_loss)\n",
    "                results[\"train_acc\"].append(epoch_acc.item())\n",
    "            else:\n",
    "                tittle = 'Validación'\n",
    "                results[\"val_loss\"].append(epoch_loss)\n",
    "                results[\"val_acc\"].append(epoch_acc.item())\n",
    "            print(f'{tittle}\\t{(time.time() - since)//60:.0f}min {(time.time() - since)%60:.0f}seg\\tPerdida: {epoch_loss:.4f}\\tPrecisión: {epoch_acc:.4f}')\n",
    "            \n",
    "            # copia del modelo con mejor resultado\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    if time_elapsed < 3600:\n",
    "        print(f'Entrenamiento completado en: {time_elapsed // 60:.0f}min. {time_elapsed % 60:.0f}seg.')\n",
    "    else:\n",
    "        rest_elapsed = time_elapsed % 3600\n",
    "        print(f'Entrenamiento completado en: {time_elapsed // 3600:.0f}horas {rest_elapsed // 60:.0f}min. {rest_elapsed % 60:.0f}seg.')\n",
    "    print(f'Mejor precisión validación: {best_acc:4f}')\n",
    "\n",
    "    # guarda los mejores pesos del modelo\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0309f80c-ab36-4f83-b8fc-34f572da595a",
   "metadata": {},
   "source": [
    "## ResNet-50 preentrenado\n",
    "\n",
    "Carga un modelo preentrenado y restablece la última capa totalmente conectada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6c688ae-a268-44a0-942d-2ef6d781adce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(weights='IMAGENET1K_V2')\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "# Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``.\n",
    "model.fc = nn.Linear(num_ftrs, 10000)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Se optimizan todos los parámetros\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Se decrementa LR por un factor 0.1 cada 7 iteraciones\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e82380ee-6c72-4039-a32b-2ebd7c5cecd2",
   "metadata": {},
   "source": [
    "## Entrenamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceb4f71c-380c-445c-9a69-5cecdcbf3c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etapa 1/20\n",
      "-----------\n",
      "Entrenamiento\t104min 24seg\tPerdida: 7.9920\tPrecisión: 0.0147\n",
      "Validación\t111min 27seg\tPerdida: 5.5578\tPrecisión: 0.0782\n",
      "\n",
      "Etapa 2/20\n",
      "-----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m train_model(model, criterion, optimizer, exp_lr_scheduler,\n\u001b[0;32m      2\u001b[0m                        num_epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[5], line 41\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[0;32m     38\u001b[0m             optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     40\u001b[0m     \u001b[39m# estatisticas\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m inputs\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     42\u001b[0m     running_corrects \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(preds \u001b[39m==\u001b[39m labels\u001b[39m.\u001b[39mdata)\n\u001b[0;32m     43\u001b[0m \u001b[39mif\u001b[39;00m phase \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train_model(model, criterion, optimizer, exp_lr_scheduler,\n",
    "                       num_epochs=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3162f29-c218-4bdc-be78-d0c09386966f",
   "metadata": {},
   "source": [
    "Parar aqui/Continuar al terminar el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bc73d89-7505-4ad5-b461-8a5ac7759708",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_results \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(results)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fdab5c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_results\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mresultsRestnetV2.txt\u001b[39m\u001b[39m'\u001b[39m, sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_results' is not defined"
     ]
    }
   ],
   "source": [
    "df_results.to_csv('resultsRestnetV2.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae232dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(results: Dict[str, List[float]]):\n",
    "    \n",
    "    # Recuperamos los valores de pérdida de entrenamiento y validación\n",
    "    loss = results['train_loss']\n",
    "    test_loss = results['val_loss']\n",
    "\n",
    "    # Recuperamos los valores de precisión de entrenamiento y validación\n",
    "    accuracy = results['train_acc']\n",
    "    test_accuracy = results['val_acc']\n",
    "\n",
    "    # Número de iteraciones que tenemos\n",
    "    epochs = range(len(results['train_loss']))\n",
    "\n",
    "    # Definimos el gráfico\n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    # Pérdida\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss, label='Pérdida entrenamiento')\n",
    "    plt.plot(epochs, test_loss, label='Pérdida validacion')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.xlabel('Iteraciones')\n",
    "    plt.legend()\n",
    "\n",
    "    # Precisión\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, accuracy, label='Precisión entrenamiento')\n",
    "    plt.plot(epochs, test_accuracy, label='Precisión validación')\n",
    "    plt.ylabel('Precisión')\n",
    "    plt.xlabel('Iteraciones')\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab126f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
