{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c9717ec-f49f-4032-829a-a4e4de409a1c",
   "metadata": {},
   "source": [
    "# EfficientNet-B3\n",
    "\n",
    "## Importamos librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08482d97-ab40-4fee-b737-32261c038f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ba7d3c-0489-43ed-8987-1efd18782129",
   "metadata": {},
   "source": [
    "Definimos varios parámetros que vamos a utilizar a lo largo del cuaderno, por ejemplo el diccionario que vamos a utilizar para alamacenar los valores de precisión y perdida que se van generando durante el entrenamiento. La resolución de a la que vamos a ajutar las imágenes. El directorio donde se almacenan los datos. Etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d726595d-d4b1-45a9-abf6-d1d599a57c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un diccionario vacio para almacenar los resultados\n",
    "results = {'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "cudnn.benchmark = True\n",
    "\n",
    "data_dir = './dataR'\n",
    "img_size = (224, 224)\n",
    "bs = 2\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "num_epochs = 20\n",
    "output_classes = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcf312e-aee0-491d-b329-e63659e38cf2",
   "metadata": {},
   "source": [
    "## Definimos el parámetro transform\n",
    "\n",
    "Definimos las diferentes transformaciones que se van a aplicar previamente sobre los datos:\n",
    "\n",
    "* Redimensionamos todas las imágenes al mismo tamaño (224x224).\n",
    "* Aleatoriamente se aplica un giro horizontal de la imagen.\n",
    "* Rotamos la imagen 10 grados de forma aleatoria.\n",
    "* Los trasformaremos en tensores y normalizamos $\\frac{x-mean}{std}$ centrado en 0 y [-1, 1].\n",
    "\n",
    "Inicialmente, vamos a aplicar la misma transformación al conjunto de datos de entrenamiento que al conjunto de datos de validación. Pero hemos escrito el código de forma que se puedan aplicar diferentes transformaciones al conjunto de datos de entrenamiento que al conjunto de datos de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fe96364-1e6b-4483-8b3f-1d265a3b3971",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07414d2e-7bc1-4c9a-b6b1-396220714bf0",
   "metadata": {},
   "source": [
    "## Carga de datos\n",
    "\n",
    "Hemos decidido utilizar el conjunto de datos del iNat Challenge 2.021 para entrenar y validar nuestro modelo. Contiene imágenes de 10.000 especies diferentes. El conjunto de datos de entrenamiento mini formado por 500.000 imágenes, que contiene 50 imágenes de cada una de las especies catalogadas. El conjunto de datos de validación contiene para cada especie 10 imágenes, sumando un total de 100.000 imágenes.\n",
    "\n",
    "80% de los datos forman parte del conjunto de datos de entrenamiento y el 20% de los datos son el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58102926-98f3-431f-9c21-86ab39dc4d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 5000, 'val': 1000}\n"
     ]
    }
   ],
   "source": [
    "image_datasets = {\n",
    "    x: datasets.INaturalist(\n",
    "        root = data_dir,\n",
    "        version = '2021_train_mini' if x == 'train' else '2021_valid',\n",
    "        transform = data_transforms[x],\n",
    "        download = False\n",
    "    )\n",
    "    for x in ['train', 'val']\n",
    "}\n",
    "dataloaders = {\n",
    "    x: torch.utils.data.DataLoader(\n",
    "        image_datasets[x],\n",
    "        batch_size = bs,\n",
    "        shuffle = True,\n",
    "        num_workers = 4\n",
    "    )\n",
    "    for x in ['train', 'val']\n",
    "}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e689afb8-4901-4ce2-92a9-f3530b9683a6",
   "metadata": {},
   "source": [
    "## Función de entrenamiento\n",
    "\n",
    "Definimos la función que nos servirá para entrenar nuestra red neuronal. Esta función devuelve como resultado el modelo con los parámetros ajustados a la ejecucion del entrenamiento en el que obtivo un mayor porcentaje de acierto en la validación del modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22993dd0-5e8f-4689-be79-67391b1ed47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=20):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Entrenando etapa {epoch + 1} de {num_epochs} ...')\n",
    "        epoch_init = time.time()\n",
    "\n",
    "        # Cada iteración tiene una fase de validación y una fase de entrenamiento\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Establece el modelo en modo entrenamiento\n",
    "            else:\n",
    "                model.eval()   # Establece el modelo en modo validación\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Itera sobre los datos\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # pone a cero los gradientes de los tensores optimizados\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # track historial adelante solo en la fase de entrenamiento\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Solo si está en la fase de entrenamiento retroceder + optimizar\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # estatisticas\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            if phase == 'train':\n",
    "                results[\"train_loss\"].append(epoch_loss)\n",
    "                results[\"train_acc\"].append(epoch_acc.item())\n",
    "                print(f'Entrenamiento\\tPerdida: {epoch_loss:.4f}\\tPrecisión: {epoch_acc:.4f}')\n",
    "            else:\n",
    "                tittle = 'Validación'\n",
    "                results[\"val_loss\"].append(epoch_loss)\n",
    "                results[\"val_acc\"].append(epoch_acc.item())\n",
    "                print(f'Validación\\tPerdida: {epoch_loss:.4f}\\tPrecisión: {epoch_acc:.4f}'\n",
    "                      f'\\t{(time.time() - epoch_init)//60:.0f}min {(time.time() - epoch_init)%60:.0f}seg')\n",
    "            \n",
    "            # copia del modelo con mejor porcentaje de acierto en la validación\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                print('Guardando el modelo ...')\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    if time_elapsed < 3600:\n",
    "        print(f'Entrenamiento completado en: {time_elapsed // 60:.0f}min. {time_elapsed % 60:.0f}seg.')\n",
    "    else:\n",
    "        rest_elapsed = time_elapsed % 3600\n",
    "        print(f'Entrenamiento completado en: {time_elapsed // 3600:.0f}horas {rest_elapsed // 60:.0f}min. {rest_elapsed % 60:.0f}seg.')\n",
    "    print(f'Mejor precisión validación: {best_acc:4f}')\n",
    "\n",
    "    # guarda los mejores pesos del modelo\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7e8cdd-e50c-4096-8859-fd154ec6444a",
   "metadata": {},
   "source": [
    "## Cargando el modelo pre-entrenado\n",
    "\n",
    "El modelo ha sido pre-entrenado con Conjunto de datos ImageNet en el que cada canal de color se normalizó por separado. Para las medias, es [0,485, 0,456, 0,406] y para las desviaciones estándar [0,229, 0,224, 0,225], calculadas a partir de las imágenes de ImageNet. Estos valores desplazarán cada canal de color para que esté centrado en 0 y oscile entre -1 y 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c72add95-c6be-4919-89c3-298e3e3ebb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.efficientnet_b5(weights=None)\n",
    "#num_ftrs = model.classifier.in_features\n",
    "\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "model.classifier = nn.Linear(num_ftrs, output_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Se optimizan todos los parámetros\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Se decrementa LR por un factor 0.1 cada 7 iteraciones\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "544dc2f5-b4e4-4c36-85cd-5a97b4e86078",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "EfficientNet                                            --\n",
       "├─Sequential: 1-1                                       --\n",
       "│    └─Conv2dNormActivation: 2-1                        --\n",
       "│    │    └─Conv2d: 3-1                                 1,296\n",
       "│    │    └─BatchNorm2d: 3-2                            96\n",
       "│    │    └─SiLU: 3-3                                   --\n",
       "│    └─Sequential: 2-2                                  --\n",
       "│    │    └─MBConv: 3-4                                 2,940\n",
       "│    │    └─MBConv: 3-5                                 1,206\n",
       "│    │    └─MBConv: 3-6                                 1,206\n",
       "│    └─Sequential: 2-3                                  --\n",
       "│    │    └─MBConv: 3-7                                 13,046\n",
       "│    │    └─MBConv: 3-8                                 27,450\n",
       "│    │    └─MBConv: 3-9                                 27,450\n",
       "│    │    └─MBConv: 3-10                                27,450\n",
       "│    │    └─MBConv: 3-11                                27,450\n",
       "│    └─Sequential: 2-4                                  --\n",
       "│    │    └─MBConv: 3-12                                37,098\n",
       "│    │    └─MBConv: 3-13                                73,104\n",
       "│    │    └─MBConv: 3-14                                73,104\n",
       "│    │    └─MBConv: 3-15                                73,104\n",
       "│    │    └─MBConv: 3-16                                73,104\n",
       "│    └─Sequential: 2-5                                  --\n",
       "│    │    └─MBConv: 3-17                                91,664\n",
       "│    │    └─MBConv: 3-18                                256,800\n",
       "│    │    └─MBConv: 3-19                                256,800\n",
       "│    │    └─MBConv: 3-20                                256,800\n",
       "│    │    └─MBConv: 3-21                                256,800\n",
       "│    │    └─MBConv: 3-22                                256,800\n",
       "│    │    └─MBConv: 3-23                                256,800\n",
       "│    └─Sequential: 2-6                                  --\n",
       "│    │    └─MBConv: 3-24                                306,048\n",
       "│    │    └─MBConv: 3-25                                496,716\n",
       "│    │    └─MBConv: 3-26                                496,716\n",
       "│    │    └─MBConv: 3-27                                496,716\n",
       "│    │    └─MBConv: 3-28                                496,716\n",
       "│    │    └─MBConv: 3-29                                496,716\n",
       "│    │    └─MBConv: 3-30                                496,716\n",
       "│    └─Sequential: 2-7                                  --\n",
       "│    │    └─MBConv: 3-31                                632,140\n",
       "│    │    └─MBConv: 3-32                                1,441,644\n",
       "│    │    └─MBConv: 3-33                                1,441,644\n",
       "│    │    └─MBConv: 3-34                                1,441,644\n",
       "│    │    └─MBConv: 3-35                                1,441,644\n",
       "│    │    └─MBConv: 3-36                                1,441,644\n",
       "│    │    └─MBConv: 3-37                                1,441,644\n",
       "│    │    └─MBConv: 3-38                                1,441,644\n",
       "│    │    └─MBConv: 3-39                                1,441,644\n",
       "│    └─Sequential: 2-8                                  --\n",
       "│    │    └─MBConv: 3-40                                1,792,268\n",
       "│    │    └─MBConv: 3-41                                3,976,320\n",
       "│    │    └─MBConv: 3-42                                3,976,320\n",
       "│    └─Conv2dNormActivation: 2-9                        --\n",
       "│    │    └─Conv2d: 3-43                                1,048,576\n",
       "│    │    └─BatchNorm2d: 3-44                           4,096\n",
       "│    │    └─SiLU: 3-45                                  --\n",
       "├─AdaptiveAvgPool2d: 1-2                                --\n",
       "├─Sequential: 1-3                                       --\n",
       "│    └─Dropout: 2-10                                    --\n",
       "│    └─Linear: 2-11                                     2,049,000\n",
       "================================================================================\n",
       "Total params: 30,389,784\n",
       "Trainable params: 30,389,784\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493f268c-478c-4352-92fe-697e24ea9f37",
   "metadata": {},
   "source": [
    "## Entrenamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0e4b7b4-1bd9-4f80-b11e-10e1d6f31061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando etapa 1 de 20 ...\n",
      "Entrenamiento\tPerdida: 4.6870\tPrecisión: 0.0116\n",
      "Validación\tPerdida: 98.1301\tPrecisión: 0.0080\t8min 47seg\n",
      "Guardando el modelo ...\n",
      "\n",
      "Entrenando etapa 2 de 20 ...\n",
      "Entrenamiento\tPerdida: 4.6560\tPrecisión: 0.0074\n",
      "Validación\tPerdida: 129.9712\tPrecisión: 0.0120\t8min 40seg\n",
      "Guardando el modelo ...\n",
      "\n",
      "Entrenando etapa 3 de 20 ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12336\\2750929960.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12336\\4035341349.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[1;31m# estatisticas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m                 \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m                 \u001b[0mrunning_corrects\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c48d8ec-bb9c-43c7-bc81-4deccf646f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.2, inplace=True)\n",
       "  (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.efficientnet_b0(weights=None)\n",
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ae4d228-8b39-4b30-b207-27d58db50329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1280, out_features=100, bias=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_ftrs = model.classifier[1].in_features\n",
    "model.classifier = nn.Linear(num_ftrs, output_classes)\n",
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b30d7d3-a61c-4475-9c96-6b11a7c378b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
